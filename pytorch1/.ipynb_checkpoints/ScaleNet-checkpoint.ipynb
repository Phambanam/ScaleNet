{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bae88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import math\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from heper_dataset import get_dataloaders_cifar10,UnNormalize\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_evaluation import set_all_seeds, set_deterministic, compute_confusion_matrix\n",
    "from helper_train import train_model\n",
    "from helper_plotting import plot_training_loss, plot_accuracy, show_examples, plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52a60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8986d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(RANDOM_SEED)\n",
    "#set_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "552ff4a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##########################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m### CIFAR-10 DATASET\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m##########################\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_transforms \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      7\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m120\u001b[39m)),\n\u001b[1;32m      8\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mRandomCrop((\u001b[38;5;241m110\u001b[39m, \u001b[38;5;241m110\u001b[39m)),\n\u001b[1;32m      9\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     10\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m     11\u001b[0m                                       ])\n\u001b[1;32m     13\u001b[0m test_transforms \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     14\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m120\u001b[39m)),        \n\u001b[1;32m     15\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCenterCrop((\u001b[38;5;241m110\u001b[39m, \u001b[38;5;241m110\u001b[39m)),            \n\u001b[1;32m     16\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),                \n\u001b[1;32m     17\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))])\n\u001b[1;32m     20\u001b[0m train_loader, valid_loader, test_loader \u001b[38;5;241m=\u001b[39m get_dataloaders_cifar10(\n\u001b[1;32m     21\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     22\u001b[0m     validation_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     23\u001b[0m     train_transforms\u001b[38;5;241m=\u001b[39mtrain_transforms,\n\u001b[1;32m     24\u001b[0m     test_transforms\u001b[38;5;241m=\u001b[39mtest_transforms,\n\u001b[1;32m     25\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### CIFAR-10 DATASET\n",
    "##########################\n",
    "\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((120, 120)),\n",
    "    torchvision.transforms.RandomCrop((110, 110)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ])\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((120, 120)),        \n",
    "    torchvision.transforms.CenterCrop((110, 110)),            \n",
    "    torchvision.transforms.ToTensor(),                \n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_fraction=0.1,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    num_workers=2)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    print('Class labels of 10 examples:', labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d25b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SABlock(nn.Module):\n",
    "    layer_idx = -1\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, bias=False, downsample=False, structure=[]):\n",
    "        super(SABlock, self).__init__()\n",
    "        if SABlock.layer_idx < len(structure):\n",
    "            SABlock.layer_idx += 1\n",
    "        channels = structure[SABlock.layer_idx][:-1]\n",
    "        side = structure[SABlock.layer_idx][-1]\n",
    "        \n",
    "        self.scales = [None, 2, 4, 7]\n",
    "        \n",
    "        self.stride = stride\n",
    "        \n",
    "\n",
    "        self.downsample = None if downsample == False else \\\n",
    "                          nn.Sequential(nn.Conv2d(inplanes, planes * SABlock.expansion, kernel_size=1, stride=1, bias=bias),\n",
    "                                        nn.BatchNorm2d(planes * SABlock.expansion))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # kernel size == 1 if featuremap size == 1\n",
    "        self.conv2 = nn.ModuleList([nn.Conv2d(planes, channels[i], kernel_size=3 if side / 2**i > 1 else 1, stride=1, padding=1 if side / 2**i > 1 else 0, bias=bias) if channels[i] > 0 else \\\n",
    "                                    None for i in range(len(self.scales))])\n",
    "        self.bn2 = nn.ModuleList([nn.BatchNorm2d(channels[i]) if channels[i] > 0 else \\\n",
    "                                  None for i in range(len(self.scales))])\n",
    "\n",
    "        self.conv3 = nn.Conv2d(sum(channels), planes * SABlock.expansion, kernel_size=1, bias=bias)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * SABlock.expansion)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(x, self.stride, self.stride) if self.stride > 1 else x\n",
    "    \n",
    "        residual = self.downsample(x) if self.downsample != None else x\n",
    "\n",
    "        out1 = self.conv1(x)\n",
    "        out1 = F.relu(self.bn1(out1))\n",
    "      \n",
    "        out2_list = []\n",
    "        size = [out1.size(2), out1.size(3)]\n",
    "        for i in range(len(self.scales)):\n",
    "            out2_i = out1 # copy\n",
    "            if self.scales[i] != None:\n",
    "                out2_i = F.max_pool2d(out2_i, self.scales[i], self.scales[i])\n",
    "            if self.conv2[i] != None:\n",
    "                out2_i = self.conv2[i](out2_i)\n",
    "            if self.scales[i] != None:\n",
    "                # nearest mode is not suitable for upsampling on non-integer multiples \n",
    "                mode = 'nearest' if size[0] % out2_i.shape[2] == 0 and size[1] % out2_i.shape[3] == 0 else 'bilinear'\n",
    "                out2_i = F.interpolate(out2_i, size=size, mode=mode)\n",
    "            if self.bn2[i] != None:\n",
    "                out2_i = self.bn2[i](out2_i)\n",
    "                out2_list.append(out2_i)\n",
    "        out2 = torch.cat(out2_list, 1)\n",
    "        out2 = F.relu(out2)\n",
    "\n",
    "        out3 = self.conv3(out2)\n",
    "        out3 = self.bn3(out3)\n",
    "        out3 += residual\n",
    "        out3 = F.relu(out3)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6e85ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, structure, num_classes=1000):\n",
    "        super(ScaleNet, self).__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.structure = structure\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            annels = structure[SABlock.layer_idx][:-1]\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = True if stride != 1 or self.inplanes != planes * block.expansion else False\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample=downsample, structure=self.structure))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, downsample=False, structure=self.structure))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.max_pool2d(x, 3, 2, 1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "66c9c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scalenet50(structure_path, ckpt=None, **kwargs):\n",
    "    layer = [3, 4, 6, 3]\n",
    "    structure = json.loads(open(structure_path).read())\n",
    "    model = ScaleNet(SABlock, layer, structure, **kwargs)\n",
    "\n",
    "    # pretrained\n",
    "    if ckpt != None:\n",
    "        state_dict = torch.load(ckpt, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5abd0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalenet101(structure_path, ckpt=None, **kwargs):\n",
    "    layer = [3, 4, 23, 3]\n",
    "    structure = json.loads(open(structure_path).read())\n",
    "    model = ScaleNet(SABlock, layer, structure, **kwargs)\n",
    "\n",
    "    # pretrained\n",
    "    if ckpt != None:\n",
    "        state_dict = torch.load(ckpt, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e301f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalenet152(structure_path, ckpt=None, **kwargs):\n",
    "    layer = [3, 8, 36, 3]\n",
    "    structure = json.loads(open(structure_path).read())\n",
    "    model = ScaleNet(SABlock, layer, structure, **kwargs)\n",
    "\n",
    "    # pretrained\n",
    "    if ckpt != None:\n",
    "        state_dict = torch.load(ckpt, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b3b151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51e41c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds)\n",
    "            ,dim=0\n",
    "        )\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c2c676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scalenet50(structure_path='../structures/scalenet50.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0de31eea",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb Cell 13'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=1'>2</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=2'>3</a>\u001b[0m                                                        factor\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=3'>4</a>\u001b[0m                                                        mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=4'>5</a>\u001b[0m                                                        verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=6'>7</a>\u001b[0m minibatch_loss_list, train_acc_list, valid_acc_list \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=7'>8</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=8'>9</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=9'>10</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=10'>11</a>\u001b[0m     valid_loader\u001b[39m=\u001b[39;49mvalid_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=11'>12</a>\u001b[0m     test_loader\u001b[39m=\u001b[39;49mtest_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=12'>13</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=13'>14</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=14'>15</a>\u001b[0m     scheduler_on\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvalid_acc\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=15'>16</a>\u001b[0m     logging_interval\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=17'>18</a>\u001b[0m plot_training_loss(minibatch_loss_list\u001b[39m=\u001b[39mminibatch_loss_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=18'>19</a>\u001b[0m                    num_epochs\u001b[39m=\u001b[39mNUM_EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=19'>20</a>\u001b[0m                    iter_per_epoch\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=20'>21</a>\u001b[0m                    results_dir\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=21'>22</a>\u001b[0m                    averaging_iterations\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phamnam/Desktop/ScaleNet/pytorch1/ScaleNet.ipynb#ch0000012?line=22'>23</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: train_model() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       mode='max',\n",
    "                                                       verbose=True)\n",
    "\n",
    "minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n",
    "    model=model,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_on='valid_acc',\n",
    "    logging_interval=100)\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=NUM_EPOCHS,\n",
    "                   iter_per_epoch=len(train_loader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=200)\n",
    "plt.show()\n",
    "\n",
    "plot_accuracy(train_acc_list=train_acc_list,\n",
    "              valid_acc_list=valid_acc_list,\n",
    "              results_dir=None)\n",
    "plt.ylim([60, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c75cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
